{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_serve_rnn_prod.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mdasadul/MNIST/blob/master/tensorflow_serve_rnn_prod.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "fWGnnJWxAlrr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I am interested to do following\n",
        "\n",
        "\n",
        "*   Train a RNN model\n",
        "*   Save the model\n",
        "*   Export the model\n",
        "*   Serve it to production using Tensorflow \n",
        "\n",
        "\n",
        "\n",
        "Let's load necessary libraries"
      ]
    },
    {
      "metadata": {
        "id": "drkDcGL276QA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "21Eh2UmkZRzK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are downloading data into a temporary location and we are converting each example of our dataset into one_hot vector. To classify images using a recurrent neural network, we consider every image row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\n",
        "handle 28 sequences of 28 steps for every sample.\n"
      ]
    },
    {
      "metadata": {
        "id": "W_nDgNloZ-9_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "09af4b40-8278-4a5b-9312-b5316ef27caf"
      },
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dlgENxxmZ8au",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are setting up training parameters and network parameters. We are keeping learning rate low and making sure the model is not going to overfit training data."
      ]
    },
    {
      "metadata": {
        "id": "3xEskJmJkCBO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "training_steps = 1000\n",
        "batch_size = 128\n",
        "display_step = 200\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 28 # MNIST data input (img shape: 28*28)\n",
        "timesteps = 28 # timesteps\n",
        "num_hidden = 128 # hidden layer num of features\n",
        "num_classes = 10 # MNIST total classes (0-9 digits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1meN6hxMkIM1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is very common in tensorflow to use placeholder and use feed_dict to feed data during training and inference. We are naming input to \"Input_X\" so that we can use it in future when we will be saving the graph and also during inferencing."
      ]
    },
    {
      "metadata": {
        "id": "kKlYb_fQn9J5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training Parameters\n",
        "\n",
        "output_layer = ['output_layer/add']\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(\"float\", [None, timesteps, num_input],name ='Input_X')\n",
        "Y = tf.placeholder(\"float\", [None, num_classes])\n",
        "\n",
        "# Define weights\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mI0l3gXVoB5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are defining our simple RNN model here. We atart by preparing data shape to match `rnn` function requirements. Then we are creating LSTM cell with  BasicLSTMCell. after that we are applying that on the input we prepared for. We did create those cell within a variable scope and set auto_reuse=true to reuse the module.  Finally we are calculating logit by applying linear activation. In the inference graph we can use this linear activation as output layer and apply softmax during inference for output. If we would like to use this linear activation layer as output we should put them into scope and assign name to the logit opearion. "
      ]
    },
    {
      "metadata": {
        "id": "MSsueWU28SBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "fb542947-dc79-46b9-a740-0201c6a21d47"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def RNN(x, weights, biases):\n",
        "\n",
        "    # Prepare data shape to match `rnn` function requirements\n",
        "    # Current data input shape: (batch_size, timesteps, n_input)\n",
        "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
        "\n",
        "    x = tf.unstack(x, timesteps, 1)\n",
        "\n",
        "    # Define a lstm cell \n",
        "    with tf.variable_scope(\"rnn\", reuse=tf.AUTO_REUSE):\n",
        "      lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "      \n",
        "      outputs, _ = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
        "\n",
        "    # Linear activation, using rnn inner loop last output\n",
        "    with tf.name_scope('output_layer'):\n",
        "        logit = tf.add(tf.matmul(outputs[-1], weights['out']) , biases['out'],name ='add')\n",
        "    return logit\n",
        "\n",
        "logits = RNN(X, weights, biases)\n",
        "prediction = tf.nn.softmax(logits,name='y_')\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluate model (with test logits, for dropout to be disabled)\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "output_tensor = tf.get_default_graph().get_tensor_by_name(\"output_layer/add:0\")\n",
        "input_tensor = tf.get_default_graph().get_tensor_by_name(\"Input_X:0\")\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, training_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # Reshape data to get 28 seq of 28 elements\n",
        "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "    #for op in tf.get_default_graph().get_operations():\n",
        "    #    if output_layer[0] in op.name:\n",
        "    #            print(op.name)\n",
        "    print(\"Optimization Finished!\")\n",
        "    saver.save(sess,'./model.ckpt')\n",
        "    # Calculate accuracy for 128 mnist test images\n",
        "    test_len = 128\n",
        "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
        "    test_label = mnist.test.labels[:test_len]\n",
        "    print(\"Testing prediction:\", \\\n",
        "        sess.run(prediction, feed_dict={X: test_data}))\n",
        "\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n",
        "    graphdef = tf.get_default_graph().as_graph_def()\n",
        "    \n",
        "    # Export the model for prediction\n",
        "    export_path =  './save'\n",
        "    shutil.rmtree(export_path)\n",
        "    builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
        "\n",
        "    tensor_info_x = tf.saved_model.utils.build_tensor_info(input_tensor)\n",
        "    #tensor_info_y = tf.saved_model.utils.build_tensor_info()\n",
        "    tensor_info_y = tf.saved_model.utils.build_tensor_info(output_tensor)\n",
        "\n",
        "    prediction_signature = (\n",
        "         tf.saved_model.signature_def_utils.build_signature_def(\n",
        "         inputs={'x_input': tensor_info_x},\n",
        "          outputs={'y_output': tensor_info_y},\n",
        "         method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
        "\n",
        "    builder.add_meta_graph_and_variables(\n",
        "       sess, [tf.saved_model.tag_constants.SERVING],\n",
        "       signature_def_map={\n",
        "      tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
        "          prediction_signature\n",
        "       },\n",
        "      )\n",
        "    builder.save()\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 2.3642, Training Accuracy= 0.102\n",
            "Step 200, Minibatch Loss= 2.0342, Training Accuracy= 0.383\n",
            "Step 400, Minibatch Loss= 1.8062, Training Accuracy= 0.469\n",
            "Step 600, Minibatch Loss= 1.7246, Training Accuracy= 0.469\n",
            "Step 800, Minibatch Loss= 1.6795, Training Accuracy= 0.500\n",
            "Step 1000, Minibatch Loss= 1.6217, Training Accuracy= 0.500\n",
            "output_layer/add\n",
            "gradients/output_layer/add_grad/Shape\n",
            "gradients/output_layer/add_grad/Shape_1\n",
            "gradients/output_layer/add_grad/BroadcastGradientArgs\n",
            "gradients/output_layer/add_grad/Sum\n",
            "gradients/output_layer/add_grad/Reshape\n",
            "gradients/output_layer/add_grad/Sum_1\n",
            "gradients/output_layer/add_grad/Reshape_1\n",
            "gradients/output_layer/add_grad/tuple/group_deps\n",
            "gradients/output_layer/add_grad/tuple/control_dependency\n",
            "gradients/output_layer/add_grad/tuple/control_dependency_1\n",
            "Optimization Finished!\n",
            "Testing prediction: [[0.00468715 0.0743486  0.00581111 ... 0.5039283  0.06872662 0.12138908]\n",
            " [0.08121999 0.01353763 0.29797387 ... 0.01006361 0.18041241 0.0291665 ]\n",
            " [0.0015001  0.7218817  0.014951   ... 0.07694507 0.02122275 0.04575193]\n",
            " ...\n",
            " [0.02510563 0.02112834 0.12790453 ... 0.11117595 0.05063821 0.13330224]\n",
            " [0.45245665 0.00471736 0.14605731 ... 0.02836649 0.01829042 0.02736905]\n",
            " [0.02147828 0.01703928 0.05430871 ... 0.11795196 0.25505272 0.14612368]]\n",
            "Testing Accuracy: 0.4609375\n",
            "INFO:tensorflow:No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: b'./save/saved_model.pb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SFFFt8NR9bf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "sess=tf.Session() \n",
        "signature_key = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n",
        "input_key = 'x_input'\n",
        "output_key = 'y_output'\n",
        "\n",
        "export_path =  './savedmodel1'\n",
        "meta_graph_def = tf.saved_model.loader.load(\n",
        "           sess,\n",
        "          [tf.saved_model.tag_constants.SERVING],\n",
        "          export_path)\n",
        "signature =  meta_graph_def.signature_def\n",
        "\n",
        "x_tensor_name = signature[signature_key].inputs[input_key].name\n",
        "y_tensor_name = signature[signature_key].outputs[output_key].name\n",
        "\n",
        "x = sess.graph.get_tensor_by_name(x_tensor_name)\n",
        "y = sess.graph.get_tensor_by_name(y_tensor_name)\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "batch_x= mnist.test.images[:1].reshape(-1,28,28)\n",
        "# Reshape data to get 28 seq of 28 elements\n",
        "batch_y = mnist.test.labels[:1][0]\n",
        "prediction = sess.run([y], feed_dict={x: batch_x})\n",
        "print(\"HERE\")\n",
        "print(batch_y)\n",
        "print(np.argmax(batch_y))\n",
        "print(np.argmax(prediction))\n",
        "correct_pred = tf.equal(np.argmax(prediction), np.argmax(batch_y))\n",
        "#accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "\n",
        "print(sess.run(correct_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}